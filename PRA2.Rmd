---
title: 'Tipologia y Ciclo de vida de los Datos. Practica 2'
author: "Autores: Alexis Arroyo y  Gabriel Pulido de Torres"
date: "Enero 2020"
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
    includes:
      in_header: PRA-header.html
  pdf_document:
    highlight: zenburn
    toc: yes
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval=T, echo=T)
```

```{r}
library(funModeling)
library(mice)
library(car)
```


******
# Introducción
******

En este conjunto de datos tenemos información acerca de los pasajeros que iban en el Titanic, naufragado en 1912 y en el que murieron 1500 personas. Además de información descriptiva del tipo de pasajero tenemos también el indicador de si sobrevivió al hundimiento o no. El objetivo es decidir si las variables aportadas son suficientes para crear un modelo predictivo que prediga la supervivencia o no de un pasajero.

******
# Descripción del dataset. ¿Por qué es importante y qué pregunta/problema pretende responder?
******

Primer contacto con el conjunto de datos, visualizamos su estructura.  

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Cargamos los paquetes R que vamos a usar
library(ggplot2)
library(dplyr)

# Cargamos el fichero de datos
data <- read.csv('train.csv',stringsAsFactors = FALSE, na.strings = "")
dim.data.frame(data)

# Verificamos la estructura del conjunto de datos
str(data)
```
El conjunto de datos incluye 12 variables y 891 observaciones.

Descripción del conjunto de datos:

Passengerld 
  Contador de pasajeros del 1 al 891.

Survived 
  esta variable toma dos valores e indica si el pasajero sobrevivió. (0= No, 1= Si).

pClass 
  clase del ticket. 1=1st (clase alta), 2=2nd (clase media) y 3=3rd (clase baja).

Name
  nombre completo del pasajero.
  
Sex
  sexo del pasajero (Female o Male).
  
Age
  edad del pasajero
  
SibSp
  número de hermanos/hermanas, hermanastros/hermanastros y marido o esposa del pasajero que también iban a bordo.

Parch
  número de hijas, hijos, padre y madre del pasajero a bordo del Titanic.
  
Ticket 
  El número del ticket del pasajero.

Fare
  Es la tarifa del pasajero en dólares.

Cabin
  Código identificativo de la cabina.
  
Embarked
  el puerto en el que embarcó el pasajero (C = Cherbourg, Q =Queenstown, S = Southampton).

## Objetivo

El objetivo es decidir si las variables aportadas son suficientes para crear un modelo predictivo que prediga la supervivencia o no de un pasajero.

# Integración y selección de los datos de interés a analizar.
******

Hemos leido los datos en un dataframe (al que llamamos "data"). Vemos su dimensionalidad, los nombres y tipos de las variables:

```{r}
dim.data.frame(data)
sapply(data, function(x) class(x))
```

Aprovechamos para comprobar si hay registros duplicados usando el comando unique()

```{r}
data_unique <- unique(data)
dim.data.frame(data_unique)
remove(data_unique)
```

Se comprueba que la dimensionalidad ha quedado exactamente igual que cuando cargamos el dataset, por lo tanto, no hay registros duplicados (nos referimos a completos duplicados).

Mostramos una muestra de los primeros registros de nuestro dataframe:

```{r}

```

Hay algunas variables que carecen de interés por ser identificativas de cada registro. Se tratan de “PassengerId”, “Ticket” y “Name”. No nos interesa tener unívocamente identificado cada caso para realizar ningún tipo de análisis, no aportan nada. 

## PersonasTicket

Antes de eliminar la variable **Ticket** nos va a servir para conocer el precio unitario que han pagado los pasajeros, ya que la variable **Fare** es la tarifa pagada en el ticket, pero dentro del mismo ticket pueden estar incluidas varias personas. Inicialmente vamos a obtener el número de personas que están incluidas en un mismo ticket.

```{r}
nrow(table(data$Ticket, data$Fare))
length(unique(data$Ticket))

data %>% group_by(Ticket, Fare) %>% filter(row_number() == 1)

```

Podemos obtener el precio por persona, dividiendo Fare / integrantes del ticket:

```{r}
ticket_personas <- as.data.frame(data %>% 
                                   group_by(Ticket) %>% 
                                   dplyr::summarize(PersonasTicket=n()))
```

Creamos un nuevo dataframe (llamado "ticket_personas") con las variables **Ticket** y **PersonasTicket** 

```{r}
df_status(ticket_personas)
```

Este nuevo dataframe lo combinamos con nuestro dataframe original (un join) a travñes del número de ticket (variable **Ticket**) para poder incorporar el número de personas al dataframe original. Después de juntarlos eliminamos el df ticket_personas pues no lo necesitaremos.

```{r}
data <- merge(data, ticket_personas, by = "Ticket")
remove(ticket_personas)
```

Ahora tenemos en nuestro dataframe original “data” una nueva variable “PersonasTicket”. Esa variable será utilizada para dividir la tarifa del ticket “Fare” entre esta nueva columna incorporada. Con ello obtenemos una nueva variable que le llamamos “Price” y es el precio por persona que se paga en el billete (obteniendo un precio por persona lineal por billete).

```{r}
data$Price <- data$Fare / data$PersonasTicket
```

Ahora ya podemos eliminar todas las variables que no vamos a necesitar: PassengerId, Name, Ticket, PersonasTicket:

```{r}
data <- select(data, -PassengerId, -Name, -Ticket, -PersonasTicket)
```

## FamilySize

Revisando **Parch** (número de Padres / hijos a bordo) y **SibSp** (sibling: número de hermanos, hermanas, hermanastros y hermanastras  del pasajero, spouse: Marido o mujer del pasajero en el titanic ) vemos que las podemos agregar en una nueva variable **FamilySize** que será la suma de estas dos variables mas el propio viajero en cuestion. Por ello esta variable como mínimo valdra uno.

```{r}
data$FamilySize = data$SibSp + data$Parch + 1
```

## Revisión y conversion de tipos

Tenemos algunas variables que, aunque a priori aparecen como “numeric” o “character” deberíamos convertir a “factor”. Estas variables son: “Survived”, “Pclass”, “Embarked” y “Sex”.
Tienen un número finito de valores y aunque puedan ser numéricas, ese número no nos aporta información con lo que los discretizamos convirtiendolos en factores:

```{r}
data$Sex <- as.factor(data$Sex)
data$Survived <- as.factor(data$Survived)
data$Pclass <- as.factor(data$Pclass)
data$Embarked <- as.factor(data$Embarked)

```

La variable **Cabin** es de tipo character, pero como vamos a eliminarla no haremos ninguna conversión.

Finalmente nos quedarían como variables numéricas: **Age**, **SubSp**, **Parch**, **FamilySize**, **Fare** y **Price**

```{r}
summary(data)
```



# Limpieza de los datos.

Analizamos los valores nulos y vacíos. Para ello nos valemos de la salida de la función *df_status*, que no s muestra un resumen del estado de nuestras 8 variables actuales:

```{r}
df_status(data)
```

## Elementos nulos y ceros. 

### Elementos nulos
########Revisar discrepancia de los datos

```{r}
data %>% group_by(Embarked) %>% count(Embarked)
```



En la tabla se observa que el campo **Age** tiene un 19.87% de nulos, **Cabin** más de un 77% y dos nulos en **Embarked**

#### Embarked

Solo hay 2 valores nulos, al ser muy pocos casos y además la variable solo toma 3 posibles valores, imputamos el valor que mas se repite, que es Embarked=S (Southampton)

```{r}
data %>% group_by(Embarked) %>% count(Embarked)
data_Embarked <- sort(table(data$Embarked, useNA = "ifany"), decreasing = TRUE)
```

Gráficamente:

```{r}
dat_plot <- as.data.frame(data_Embarked)
ggplot(dat_plot, aes(x=Var1, y=Freq, fill=Var1)) +
  geom_bar(stat = "identity", color = "black") +
  geom_text(aes(label=Freq), vjust = -0.4, color="black", size=3) +
  labs(x='Embarked', y='Count') + 
  theme(legend.position = "none")
```

Asignamos el valor mas frecuente a los casos nulos:

```{r}
data$Embarked[is.na(data$Embarked)] <- names(data_Embarked[1])
```

#### Cabin

Dado que el número de nulos es muy elevado, un 77%, se opta por eliminar esta variable ya que tendríamos que imputar valores a una parte muy importante del dataset (con su consiguiente error). Además no parece que pueda ser una variable determinante para predecir la supervivencia.

```{r}
data <- select(data, -Cabin)
```


#### Age

Tiene mas del un 19% de calores nulos (concretamente 177 registros)

Una opción para solucionarlos sería calcular la media del resto de registros e imputarla, pero vamos a estudiar si podemos delimitar la media a aplicar para obtener un resultado más depurado.

Primero analizamos la distribución de la variable **Age** teniendo en cuenta sólo los registros donde hay valores (es decir algo mas del 80% del dataframe).
Para ello creamos un nuevo dataset sin los registros nulos de Age

```{r}
data_NoNA = data[which(!is.na(data$Age)),]
```

Observamos gráficamente como se distribuye la variable Age en este dataset:

```{r}
ggplot(data_NoNA, aes(Age))+
  geom_histogram(aes(y = ..density..), bins=50, fill="steelblue", color="blue") + 
  stat_function(fun = dnorm, args = list(mean = mean(data_NoNA$Age), sd = sd(data_NoNA$Age)))
```

Comprobamos si hay alguna relación entre la edad alguna del resto de variables, para tenerlo en cuenta a la hora de imputar valores

#### Age vs Sex

Comprobamos gráficamente la relación entre la edad y el género

```{r}
titulo <- 'Age vs Sex'
ggplot(data_NoNA, aes(y=Age, x=Sex, fill=Sex)) + geom_boxplot() + labs(title = paste0('Boxplot: ', titulo)) + ylab("Age") + xlab("Sex")
```

No se aprecian casi diferencias de edad en función del género

#### Age vs Embarked

Comprobamos gráficamente la relación entre la edad y el puerto de Embarque

```{r}
titulo <- 'Age vs Embarked'
ggplot(data_NoNA, aes(y=Age, x=Embarked, fill=Embarked)) + 
  geom_boxplot() + labs(title = paste0('Boxplot: ', titulo)) + ylab("Age") + xlab("Embarked")
```

Tampoco se aprecian diferencias significativas en este caso con respecto a las medias de edad en cada una de las clases de Embarked.

#### Age vs Pclass

Representamos visualmente la relación entre Age y Pclass:

```{r}
titulo <- 'Age vs Embarked'
ggplot(data_NoNA, aes(y=Age, x=Pclass, fill=Pclass)) + 
  geom_boxplot() + labs(title = paste0('Boxplot: ', titulo)) + ylab("Age") + xlab("Pclass")
```

En este caso si se observa una relación entre la edad y la clase en que viajaban los pasajeros: Los pasajeros de clase 1 (alta) tenían generalmente mayor edad que los de clase 2 (media) e igualmente sucede con los de clase 3 (baja). 

#### Age vs Variables numéricas
######### código para la correlación
```{r}

```

Se observa que existe correlación negativa con **SibSp** y con **Parch** (también con **FamilySize**, pero eso es completamente normal porque **FamilySize** es una transformación lineal de las otras 2). También hay correlación positiva con **Price**, pero entendemos que podemos tener en cuenta la parte familiar por el tema de esposa, hijos, hermanos, etc. puede tener efecto en la edad.

#### Imputación de valores a Age

Tras el análisis de las diversas relaciones entre las variables y Age vamos a realizar cuatro simulaciones diferentes de imputación y nos quedaremos con una sola:

- Caso 1: Imputar la media de **Age** a todos los elementos faltantes

```{r}
data$Age1 <- data$Age
data$Age1[is.na(data$Age1)] <- mean(data_NoNA$Age)

ggplot(data, aes(Age1)) +
  geom_histogram(aes(y = ..density..), bins=50, fill="steelblue", color="blue") + 
  stat_function(fun = dnorm, args = list(mean = mean(data$Age1), sd = sd(data$Age1)))
```


- Caso 2: Imputar la media de **Age** pero por cada una de las clases(“Pclass”) ya que hemos visto que hay una relación entre ambas variables.

```{r}
data$Age2 <- data$Age
data$Age2[is.na(data$Age2)&data$Pclass == 1] <- mean(data$Age2[!is.na(data$Age2)&data$Pclass == 1])
data$Age2[is.na(data$Age2)&data$Pclass == 2] <- mean(data$Age2[!is.na(data$Age2)&data$Pclass == 2])
data$Age2[is.na(data$Age2)&data$Pclass == 3] <- mean(data$Age2[!is.na(data$Age2)&data$Pclass == 3])

ggplot(data, aes(Age2)) +
  geom_histogram(aes(y = ..density..), bins=50, fill="steelblue", color="blue") + 
  stat_function(fun = dnorm, args = list(mean = mean(data$Age2), sd = sd(data$Age2)))
```

- Caso 3: Imputar la datos en **Age**, pero teniendo en cuenta **Pclass**, **Parch** y **SibSp**, ya que hemos visto correlación entre las variables. Para este caso lo que hacemos es tener en cuenta las 3 variables y generar una agrupación de datos calculando la media para las combinaciones (recordemos que son variables discretas). Una vez que obtenemos esos valores medios, los imputamos. Luego verificamos si quedó algún valor sin imputar (que serán muy pocos) y para esos pocos casos faltantes, imputar los valores por cada clase.

```{r}
medias_clase_fam <- as.data.frame(data_NoNA %>% group_by(Pclass, SibSp, Parch) %>% dplyr::summarize(Media_clase_fam = mean(Age)))
```

Hacemos un meMerge entre nuestro dataset original y el de Medias (por Pclass, SIbSp y Parch) a través de las columnas usadas en la agrupación.
El merge e sun LEFT JOIN ya que puede que no existan todas las combinaciones en medias_clase_fam

```{r}
data$Age3 <- data$Age
data <- merge(data, medias_clase_fam, by = c("Pclass", "SibSp", "Parch"), all.x = TRUE)
data$Age3[is.na(data$Age3)] <- data$Media_clase_fam[is.na(data$Age3)]
```

Como es posible que nos hayan quedado alguno sin poder asignar (al no existir la combinación Pclass + sibSp + Parch) a los que faltan (que son 7) les asignamos directamente por la Pclass sin tener en cuenta los otros valores

```{r}
data$Age3[is.na(data$Age3)&data$Pclass == 1] <- mean(data$Age3[!is.na(data$Age3)&data$Pclass == 1])
data$Age3[is.na(data$Age3)&data$Pclass == 2] <- mean(data$Age3[!is.na(data$Age3)&data$Pclass == 2])
data$Age3[is.na(data$Age3)&data$Pclass == 3] <- mean(data$Age3[!is.na(data$Age3)&data$Pclass == 3])
```

Vemos ahora la distribución que nos queda

```{r}
ggplot(data, aes(Age3)) +
  geom_histogram(aes(y = ..density..), bins=50, fill="steelblue", color="blue") + 
  stat_function(fun = dnorm, args = list(mean = mean(data$Age3), sd = sd(data$Age3)))
```

- Caso 4: Imputando datos de **Age**, con MICE (Multivariate Imputation via Chained Equations). En este caso se predicen los valores de **Age**, con el resto de valores observados (usamos para este caso **Pclass**, **SibSp**, **Parch**, **Sex** y **Age**).

```{r}
columnas <- c('Pclass', 'SibSp', 'Parch', 'Sex', 'Age')

mice_imputar <- mice(data = data[, columnas], method = "rf")
#mice_imputar <-  parlmice(data = data[, columnas], method = "rf", n.core = 8, n.imp.core = 50)
mice_completo <- mice::complete(mice_imputar)
data$Age4 <- data$Age
data$Age4[is.na(data$Age4)]<- mice_completo$Age[is.na(data$Age4)]
```

Tras la imputación observamos la gráfica:

```{r}
ggplot(data, aes(Age4)) +
  geom_histogram(aes(y = ..density..), bins=50, fill="steelblue", color="blue") + 
  stat_function(fun = dnorm, args = list(mean = mean(data$Age4), sd = sd(data$Age4)))
```

Resumiendo el resultado de las 4 opciones de imputación mas la original:

```{r}
summary(data$Age)
summary(data$Age1)
summary(data$Age2)
summary(data$Age3)
summary(data$Age4)
```

Tras ver los resultados nos decantamos por la cuarta opción (Age4), ya que la distribución se parece mucho más
a la original.
Asignamos y limpiamos el dataset:

```{r}
data$Age <- data$Age4
data <- select(data, -Age1, -Age2, -Age3, -Age4, -Media_clase_fam)
```


### Ceros 

Recordamos los valores con ceros del dataset

```{r}
df_status(data)
```


Tenemos un alto número de ceros en **SibSp** y **Parch**, pero son valores válidos para estas variables, pues cuentan el número de acompañantes (familiares) del pasajero. Hay también un porcentaje  pequeño de ceros en **Fare** (tarifa), esto podría tener algún sentido (por ejemplo, tickets sin coste por ser un premio o un regalo) por lo que, en principio, vamos a dejar presentes estos ceros.


### Conclusión limpieza nulos y ceros

Viendo nuevamente los resultados con **summary**, hemos eliminado los nulos y tenemos un nuevo valor de media y mediana para **Age**

```{r}
summary(data)
```

Y con “df_status” vemos también como nos han quedado los datos después de eliminar variables, y de imputar valores en las variables que le faltaban algunos valores. El dataset queda libre de nulos, y con los ceros que hemos
aceptado que tiene que mantener y que tienen sentido.

```{r}
df_status(data)
```

## Identificación y tratamiento de valores extremos.

Se considera un valor extremo, outlier, a un valor fuera de rango. Son valores que se salen de la escala esperada visualizando el resto de las observaciones. En la actualidad, el criterio más habitual es considerar un valor extremo
a aquel que se encuentra alejado de la media unas tres veces la desviación típica.
En nuestro caso vamos a comenzar por separar aquellas variables que son numéricas y verlas en un gráfico bloxplot:

#########################Gráfico boxplot variables numéricas
```{r}

```

De todas las variables numéricas sólo 3 son contínuas: **Age**, **Price** y **Fare**

Analicemos esas tres variables por separado

### Fare

El boxplot parece indicar que Fare tiene valores extremos. Los identificamos usando el criterio de tres veces la desviación típica:

```{r}
data_out <- as.data.frame(data$Fare)
data_out$outlier <- FALSE
for (i in 1:ncol(data_out) - 1){
  columna = data_out[, i]
  if (is.numeric(columna)) {
    media = mean(columna)
    desviacion = sd(columna)
    data_out$outlier = (columna> (media+3*desviacion) | columna<(media-3*desviacion))
  }
}
table(data_out$outlier)
```

Con este criterio tenemos identificados 20 posibles outliers, observando mas detenidamente los valores que nos indica boxplot.stats y teniend en cuenta que el máximo es 512, no parecen exagerados:

```{r}
boxplot.stats(data$Fare)$out
```

Los valores, además, están bien distribuidos, los más altos en las clases altas.

Por todo esto, decidimos no realizar ninguna acción con estos outliers ya que incluso ueden estar aportando información importante:

TODO################## Gráfico outlier Fare
```{r}

```

### Price

Hacemos un análisis similar al realizado con  Fare:
```{r}
data_out <- as.data.frame(data$Price)
data_out$outlier <- FALSE
for (i in 1:ncol(data_out) - 1){
  columna = data_out[, i]
  if (is.numeric(columna)) {
    media = mean(columna)
    desviacion = sd(columna)
    data_out$outlier = (columna> (media+3*desviacion) | columna<(media-3*desviacion))
  }
}
table(data_out$outlier)
```

De nuevo con boxplot.stats analizamos los outliers, el valor máximo 221:

```{r}
boxplot.stats(data$Price)$out
```

Y al igual que con **Fare**, los valores más altos en las clases altas:
#############Gráfico boxplot Fare
```{r}

```

Al igual que con **Fare** optamos por considerarlos valores válidos y no realizamos ninguna accion.

### Age

De nuevo realizamos el mismo procedimiento que con las otras dos variables

```{r}
data_out <- as.data.frame(data$Age)
data_out$outlier <- FALSE
for (i in 1:ncol(data_out) - 1){
  columna = data_out[, i]
  if (is.numeric(columna)) {
    media = mean(columna)
    desviacion = sd(columna)
    data_out$outlier = (columna> (media+3*desviacion) | columna<(media-3*desviacion))
  }
}
table(data_out$outlier)
```

y el resultado de boxplot.stats


```{r}
boxplot.stats(data$Age)$out
```

Donde vemos que podríamos tener 2 valores outliers, pero observando los valores devueltos por boxplot.stats que son totalmente normales no los vamos a considerar outliers.

******
# Análisis de los datos.

## Selección de los grupos de datos que se quieren analizar/comparar (planificación de los análisis a aplicar).

Del dataset completo nos interesa poder realizar diferentes análisis en función de diferentes subconjuntos de datos, como puede ser el género, la clase en la que viajan los pasajeros, el puerto en el que han embarcado, incluso
se pueden definir grupos por edad, y ver realmente si es cierto y se cumplió aquello que dicen en las películas “las mujeres y los niños primero” y poder comprobar si realmente los niños tienen mejor índice de supervivencia que los
adultos.
Podemos definir diferentes agrupaciones que podemos usar más adelante para estudiar los casos por grupos

### Niños

Vamos a definir una variable Child para aquellso registros en los que la edad sea menor que 8

```{r}
edad_corte = 8
data$Child[data$Age <= edad_corte] <- 1
data$Child[data$Age > edad_corte] <- 0
data$Child <- as.factor((data$Child))
```

### Género

Agrupamos por el género, creando una variable para cada uno de los géneros

```{r}
Mujeres <- data[which(data$Sex == 'female'),]
Hombres <- data[which(data$Sex == 'male'),]
```


### Lugar de embarque

Agrupamos por el lugar de embarque creando una variable por cada uno de los lugares

```{r}
EmbarqueC <- data[which(data$Embarked == 'C'),]
EmbarqueQ <- data[which(data$Embarked == 'Q'),]
EmbarqueS <- data[which(data$Embarked == 'S'),]

```

### Clase
Agrupamos por clase

```{r}
FirstClass <- data[which(data$Pclass == 1),]
SecondClass <- data[which(data$Pclass == 2),]
ThirdClass <- data[which(data$Pclass == 3),]
```

### Edades

Vamos a discretizar los valores por grupos de edades, añadiendo una columan al dataset (AgeInterval)

```{r}
data["AgeInterval"] <- cut(data$Age, breaks = c(0,13,18,40,55, 100), labels = c("0-12", "13-17", "18-39","40-54", "55-100"))
#intervalos_edad <- c(0,13,18,40,55)
#data$AgeInterval <- findInterval(data$Age, intervalos_edad)
data$AgeInterval <- as.factor(data$AgeInterval)
table(data$AgeInterval)
plot(data$AgeInterval)
```


## Comprobación de la normalidad y homogeneidad de la varianza.

### Normalidad

Vamos a verificar si las variables cuantitativas continuas siguen una distribución normal. Algunos test estadísticos requieren que las variables que van a ser analizadas sigan una distribución normal, por tanto tenemos que conocer cuáles son las distribuciones de nuestras variables continuas.
Las únicas variables cuantitativas continuas que tenemos en el dataset original son las variables **Age** y **Fare**. Además, hemos generado una nueva variable a partir de **Fare**, que hemos llamado **Price**, y que, por tanto también es una variable cualitativa continua. 

En general, la prueba de *Shapiro-Wilk* se considera una prueba muy potente para contrastar la normalidad de distribuciones. Se asume como hipótesis nula que la población sigue una distribución normal. Si el p-valor obtenido es inferior al nivel de significancia (normalmente α = 0,05) entonces se rechaza la hipótesis nula (y por tanto se concluye que los datos no vienen de una distribución normal). En cambio, si el p-valor es superior al nivel de significancia,
entonces no se puede rechazar la hipótesis nula y se asume que los datos
siguen una distribución normal.
Para poder tener más seguridad, vamos a aplicar otros dos métodos, la prueba de *Anderson-Darling* y la prueba de *Kolmogorov-Smirnov* (conocida también como K-S)

Creamos un data frame para resumir los tests:

```{r}
tabla.normalidad <- data.frame('variable' = character(),
                               'Test de Normalidad' = character(),
                               'Valor Estadístico' = numeric(),
                               'p-value' = numeric(),
                               stringsAsFactors = FALSE) 
str(tabla.normalidad)
```
Ahora recorremos todas las variables continuas aplicando los tres tests y añadiéndolos al dataframe:

```{r}
var.continuas <-c("Age", "Fare", "Price")
library(nortest)
for (i in 1:length(var.continuas)){
  variable = var.continuas[i]
  #Test Shapiro-wil
  test = shapiro.test(data[,variable])
  tabla.normalidad[nrow(tabla.normalidad)+1,] = c(variable, test$method, test$statistic, test$p.value)
  
  #Test Anderson-Darling
  test = ad.test(data[,variable])
  tabla.normalidad[nrow(tabla.normalidad)+1,] = c(variable, test$method, test$statistic, test$p.value)
  
  #Test Kolmogorov-Smirnov
  test = ks.test(data[,variable], "pnorm", mean=mean(data[,variable]), sd=sd(data[,variable]))
  tabla.normalidad[nrow(tabla.normalidad)+1,] = c(variable, test$method, test$statistic, test$p.value)
}

knitr::kable(tabla.normalidad)
```

Con estos resultados se puede decir que ninguna de las 3 variables sigue una distribución normal, en todos los casos el p-value ha sido inferior a 0.05 y
por tanto se han rechazado la hipótesis nula (que la variable sigue una distribución normal).

No obstante, vamos a revisar gráficamente la distribución de cada una de las variables usando su histograma, curva de densidad y gráficas Q-Q

#########################Graficas histogŕamas y q-q

### Homocedasticidad (comprobación de varianzas)

Cuando comparamos varianzas lo que estamos comprobando es que las varianzas entre los grupos a comparar son iguales.
Si los datos siguen una distribución normal, podemos usar el test de Levene, en caso contrario podemos usar por ejemplo el test de Fligner-Killeen, que es la alternativa no paramétrica que se utiliza cuando los datos no siguen una distribución normal ( o cuando hay problemas con outliers no resueltos)

En ambos test (Levene y Fligner-Killeen), la hipótesis nula asume la igualdad de varianzas en los diferentes grupos de datos, con lo que si el p-valor obtenido es inferior al nivel de significancia (generalmente alpha = 0,05) se rechaza la hipótesis nula y se concluye que hay heterocedasticidad.

#### Age

La variable Age está próxima a una distribución normal por lo cual podemos utilizar el test de Levene para la comprobación de varianzas.


##### Age con Survived

Primero analizamos si las varianzas son iguales cuando comprobamos **Age** y el grupo es **Survived**, es decir estamos comprobando la homogeneidad de varianzas de la edad en los grupos de supervivientes y no supervivientes.

```{r}

leveneTest(data = data, Age ~ Survived, center = mean)
```
En este caso el p-value es superior a 0.05 y por tanto asumimos que hay homogeneidad de varianzas entre los grupos


TODO############# Representación muestras forma gráfica

##### Age con Embarked

Ahora Comprobamos también cómo se comportan las varianzas cuando se trata de la variable edad “Age” con “Embarked”.

```{r}
leveneTest(data = data, Age ~ Embarked, center = mean)
```
En este caso hay omogeneidad de varianzas de Age en los grupos que define la variable Embarked

##### Age con Pclass
Ahora Comprobamos también cómo se comportan las varianzas cuando se trata de la variable edad “Age” con “Pclass”.
En este caso vamos a aplicar tanto el test de Levene como el de Fligner-Killeen:


```{r}
leveneTest(data = data, Age ~ Pclass, center = mean)
fligner.test(Age ~ Pclass, data = data)
```
En los dos tests (parmétrico y no paramétrico), se rechaza la hipótesis nula, con lo que hay heterogeneidad d varianzas entre las muestras de **Age**  cuando se agrupan por **Pclass**

##### Age con Sex

```{r}
leveneTest(data = data, Age ~ Sex, center = mean)
fligner.test(Age ~ Sex, data = data)
```

En los dos tests el p-value es claramente superior a 0.05 por lo que podemos afirmar que sí hay homogeneidad de varianzas para *Age* cuando está agrupado por *Sex*

#### Fare

Hacemos el mismo estudio con la variable Fare pero usando Fligner

```{r}
fligner.test(Fare ~ Sex, data = data)
fligner.test(Fare ~ Survived, data = data)
fligner.test(Fare ~ Embarked, data = data)
fligner.test(Fare ~ Pclass, data = data)
```

Comprobamos la variable “Fare”, los resultados indican que hay hetero-
geneidad de varianza de esas variables respecto a los grupos con los que se
ha aplicado el test no paramétrico.

¿?
#### Price





## Aplicación de pruebas estadísticas para comparar los grupos de datos. 

En función de los datos y el objetivo del estudio, aplicar pruebas de contraste de hipótesis,
correlaciones, regresiones, etc. Aplicar al menos tres métodos de análisis diferentes.

******
# Representación de los resultados a partir de tablas y gráficas.
******
# Resolución del problema. A partir de los resultados obtenidos, ¿cuáles son las conclusiones? ¿Los resultados permiten responder al problema?



